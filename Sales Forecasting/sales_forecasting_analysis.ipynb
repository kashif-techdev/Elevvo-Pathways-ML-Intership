{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìà Sales Forecasting Project with Mandatory Bonus Features\n",
        "\n",
        "## Walmart Sales Forecast Implementation\n",
        "\n",
        "This notebook implements a comprehensive sales forecasting system with:\n",
        "- ‚úÖ Time-based feature engineering\n",
        "- ‚úÖ Lag features and rolling averages (bonus)\n",
        "- ‚úÖ Seasonal decomposition (bonus)\n",
        "- ‚úÖ Multiple ML models including XGBoost and LightGBM (bonus)\n",
        "- ‚úÖ Time-aware cross-validation (bonus)\n",
        "- ‚úÖ Comprehensive visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning imports\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Time series analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Dataset Setup\n",
        "\n",
        "For this demonstration, we'll create a synthetic dataset that mimics the Walmart sales structure. In a real scenario, you would download the actual Walmart dataset from Kaggle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic Walmart-like dataset\n",
        "print(\"üìä Creating synthetic Walmart-like dataset...\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_stores = 5\n",
        "n_depts = 3\n",
        "n_weeks = 150\n",
        "\n",
        "data = []\n",
        "for store in range(1, n_stores + 1):\n",
        "    for dept in range(1, n_depts + 1):\n",
        "        for week in range(n_weeks):\n",
        "            date = pd.date_range('2020-01-01', periods=n_weeks, freq='W')[week]\n",
        "            \n",
        "            # Create realistic sales pattern with seasonality\n",
        "            base_sales = 10000 + store * 2000 + dept * 1000\n",
        "            seasonal = 2000 * np.sin(2 * np.pi * week / 52)  # Yearly seasonality\n",
        "            trend = week * 10  # Slight upward trend\n",
        "            noise = np.random.normal(0, 1000)\n",
        "            \n",
        "            weekly_sales = max(0, base_sales + seasonal + trend + noise)\n",
        "            \n",
        "            # Add holiday effects\n",
        "            is_holiday = (week % 52 in [45, 46, 47, 48]) or (week % 52 in [0, 1, 2, 3])\n",
        "            if is_holiday:\n",
        "                weekly_sales *= 1.3\n",
        "            \n",
        "            data.append({\n",
        "                'Store': store,\n",
        "                'Dept': dept,\n",
        "                'Date': date,\n",
        "                'Weekly_Sales': weekly_sales,\n",
        "                'IsHoliday': is_holiday,\n",
        "                'Size': np.random.choice([10000, 20000, 30000]),\n",
        "                'Type': np.random.choice(['A', 'B', 'C']),\n",
        "                'Temperature': np.random.normal(70, 20),\n",
        "                'Fuel_Price': np.random.normal(3.5, 0.5),\n",
        "                'CPI': np.random.normal(200, 20),\n",
        "                'Unemployment': np.random.normal(8, 2)\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"‚úÖ Synthetic dataset created: {df.shape}\")\n",
        "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing\n",
        "\n",
        "Convert dates, sort data, and handle missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Date column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Sort data by date for each store/department\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "missing_info = df.isnull().sum()\n",
        "print(missing_info[missing_info > 0])\n",
        "\n",
        "# Handle missing values\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Feature Engineering (Core + Bonus Features)\n",
        "\n",
        "This is the heart of the forecasting system. We'll create:\n",
        "- Time-based features\n",
        "- Lag features (previous sales)\n",
        "- **Rolling averages (BONUS)**\n",
        "- **Seasonal decomposition (BONUS)**\n",
        "- Additional statistical features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Creating comprehensive features...\")\n",
        "\n",
        "# Make a copy to avoid modifying original\n",
        "df_features = df.copy()\n",
        "\n",
        "# Time-based features\n",
        "df_features['year'] = df_features['Date'].dt.year\n",
        "df_features['month'] = df_features['Date'].dt.month\n",
        "df_features['week'] = df_features['Date'].dt.isocalendar().week\n",
        "df_features['dayofweek'] = df_features['Date'].dt.dayofweek\n",
        "df_features['is_weekend'] = df_features['dayofweek'].isin([5, 6]).astype(int)\n",
        "df_features['quarter'] = df_features['Date'].dt.quarter\n",
        "\n",
        "print(\"‚úÖ Time-based features created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lag features (previous sales)\n",
        "df_features['lag_1'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
        "df_features['lag_2'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
        "df_features['lag_3'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(3)\n",
        "df_features['lag_4'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(4)\n",
        "\n",
        "print(\"‚úÖ Lag features created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rolling averages (BONUS FEATURE)\n",
        "df_features['rolling_mean_3'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=3).mean()\n",
        "df_features['rolling_mean_7'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=7).mean()\n",
        "df_features['rolling_std_3'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=3).std()\n",
        "df_features['rolling_std_7'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=7).std()\n",
        "\n",
        "# Rolling min/max\n",
        "df_features['rolling_min_3'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=3).min()\n",
        "df_features['rolling_max_3'] = df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=3).max()\n",
        "\n",
        "print(\"‚úÖ Rolling statistical features created (BONUS)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seasonal decomposition (BONUS FEATURE)\n",
        "print(\"üîß Performing seasonal decomposition...\")\n",
        "\n",
        "seasonal_features = []\n",
        "\n",
        "# Apply seasonal decomposition for each store-dept combination\n",
        "for (store, dept), group in df_features.groupby(['Store', 'Dept']):\n",
        "    if len(group) >= 52:  # Need at least 52 weeks for yearly seasonality\n",
        "        try:\n",
        "            # Perform seasonal decomposition\n",
        "            result = seasonal_decompose(group['Weekly_Sales'], model='additive', period=52)\n",
        "            \n",
        "            # Add to dataframe\n",
        "            group['trend'] = result.trend\n",
        "            group['seasonal'] = result.seasonal\n",
        "            group['residual'] = result.resid\n",
        "            \n",
        "            seasonal_features.append(group)\n",
        "        except:\n",
        "            # If decomposition fails, add zeros\n",
        "            group['trend'] = 0\n",
        "            group['seasonal'] = 0\n",
        "            group['residual'] = 0\n",
        "            seasonal_features.append(group)\n",
        "    else:\n",
        "        # Not enough data for seasonal decomposition\n",
        "        group['trend'] = 0\n",
        "        group['seasonal'] = 0\n",
        "        group['residual'] = 0\n",
        "        seasonal_features.append(group)\n",
        "\n",
        "df_features = pd.concat(seasonal_features, ignore_index=True)\n",
        "print(\"‚úÖ Seasonal decomposition completed (BONUS)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional features\n",
        "df_features['sales_ratio'] = df_features['Weekly_Sales'] / df_features.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n",
        "df_features['price_ratio'] = df_features['CPI'] / df_features.groupby(['Store'])['CPI'].transform('mean')\n",
        "\n",
        "# Holiday features\n",
        "df_features['is_holiday'] = df_features['IsHoliday'].astype(int)\n",
        "\n",
        "# Store and department features\n",
        "df_features['store_size_category'] = pd.cut(df_features['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "df_features['store_size_category'] = df_features['store_size_category'].astype('category').cat.codes\n",
        "\n",
        "# Type encoding\n",
        "df_features['type_encoded'] = df_features['Type'].astype('category').cat.codes\n",
        "\n",
        "# Remove rows with NaN values created by lag features\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "print(f\"‚úÖ Additional features created\")\n",
        "print(f\"Final features dataset shape: {df_features.shape}\")\n",
        "print(f\"Features created: {len(df_features.columns) - len(df.columns)} new features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Time-Aware Data Splitting\n",
        "\n",
        "Split data chronologically (not randomly) to avoid data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_cols = [col for col in df_features.columns if col not in [\n",
        "    'Date', 'Weekly_Sales', 'Store', 'Dept', 'Type', 'IsHoliday'\n",
        "]]\n",
        "\n",
        "X = df_features[feature_cols]\n",
        "y = df_features['Weekly_Sales']\n",
        "\n",
        "print(f\"Features used: {len(feature_cols)}\")\n",
        "print(f\"Feature columns: {feature_cols}\")\n",
        "\n",
        "# Time-aware split (80-20)\n",
        "split_idx = int(len(df_features) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "test_dates = df_features[split_idx:]['Date']\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Training date range: {df_features[:split_idx]['Date'].min()} to {df_features[:split_idx]['Date'].max()}\")\n",
        "print(f\"Test date range: {test_dates.min()} to {test_dates.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Baseline Models\n",
        "\n",
        "Train Linear Regression and Random Forest models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ü§ñ Training baseline models...\")\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Linear Regression\n",
        "print(\"Training Linear Regression...\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "models['LinearRegression'] = lr_model\n",
        "\n",
        "# Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "models['RandomForest'] = rf_model\n",
        "\n",
        "print(\"‚úÖ Baseline models trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Advanced Models (BONUS FEATURES)\n",
        "\n",
        "Train XGBoost and LightGBM models with advanced configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üöÄ Training advanced models (BONUS)...\")\n",
        "\n",
        "# XGBoost (BONUS)\n",
        "print(\"Training XGBoost...\")\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "models['XGBoost'] = xgb_model\n",
        "\n",
        "# LightGBM (BONUS)\n",
        "print(\"Training LightGBM...\")\n",
        "lgb_model = LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    feature_fraction=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "models['LightGBM'] = lgb_model\n",
        "\n",
        "print(\"‚úÖ Advanced models trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Time-Aware Cross-Validation (BONUS)\n",
        "\n",
        "Perform time-aware cross-validation to properly evaluate time series models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_aware_cross_validation(X_train, y_train, model_name='XGBoost'):\n",
        "    \"\"\"\n",
        "    Time-aware cross-validation (BONUS FEATURE)\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Performing time-aware cross-validation for {model_name}...\")\n",
        "    \n",
        "    # Initialize model\n",
        "    if model_name == 'XGBoost':\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=6,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    elif model_name == 'LightGBM':\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            num_leaves=31,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1\n",
        "        )\n",
        "    else:\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    \n",
        "    # Time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    cv_scores = []\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        print(f\"  Fold {fold + 1}/5\")\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_tr, y_tr)\n",
        "        \n",
        "        # Predict and evaluate\n",
        "        y_pred = model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "        cv_scores.append(mae)\n",
        "        \n",
        "        print(f\"    MAE: {mae:.2f}\")\n",
        "    \n",
        "    avg_mae = np.mean(cv_scores)\n",
        "    print(f\"Average MAE across folds: {avg_mae:.2f}\")\n",
        "    \n",
        "    return cv_scores\n",
        "\n",
        "# Perform time-aware cross-validation\n",
        "print(\"\\nüîÑ Time-aware Cross-Validation Results:\")\n",
        "xgb_cv_scores = time_aware_cross_validation(X_train, y_train, 'XGBoost')\n",
        "lgb_cv_scores = time_aware_cross_validation(X_train, y_train, 'LightGBM')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Model Evaluation and Visualization\n",
        "\n",
        "Evaluate all models and create comprehensive visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Evaluating models...\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    results[model_name] = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R2': r2,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"  MAE: {mae:.2f}\")\n",
        "    print(f\"  RMSE: {rmse:.2f}\")\n",
        "    print(f\"  R¬≤: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "print(\"üìà Creating visualizations...\")\n",
        "\n",
        "# Set up the plotting style\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "fig.suptitle('Sales Forecasting Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Actual vs Predicted for all models\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(test_dates, y_test, label='Actual', color='blue', linewidth=2)\n",
        "\n",
        "colors = ['red', 'green', 'orange', 'purple']\n",
        "for i, (model_name, result) in enumerate(results.items()):\n",
        "    ax1.plot(test_dates, result['predictions'], \n",
        "            label=f'{model_name} (MAE: {result[\"MAE\"]:.0f})', \n",
        "            color=colors[i], alpha=0.7, linewidth=1.5)\n",
        "\n",
        "ax1.set_title('Actual vs Predicted Sales')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Weekly Sales')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Model comparison bar chart\n",
        "ax2 = axes[0, 1]\n",
        "model_names = list(results.keys())\n",
        "mae_scores = [results[name]['MAE'] for name in model_names]\n",
        "rmse_scores = [results[name]['RMSE'] for name in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "ax2.bar(x - width/2, mae_scores, width, label='MAE', alpha=0.8)\n",
        "ax2.bar(x + width/2, rmse_scores, width, label='RMSE', alpha=0.8)\n",
        "\n",
        "ax2.set_title('Model Performance Comparison')\n",
        "ax2.set_xlabel('Models')\n",
        "ax2.set_ylabel('Error')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(model_names, rotation=45)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Residuals plot for best model\n",
        "best_model = min(results.keys(), key=lambda x: results[x]['MAE'])\n",
        "ax3 = axes[1, 0]\n",
        "residuals = y_test - results[best_model]['predictions']\n",
        "ax3.scatter(results[best_model]['predictions'], residuals, alpha=0.6)\n",
        "ax3.axhline(y=0, color='red', linestyle='--')\n",
        "ax3.set_title(f'Residuals Plot - {best_model}')\n",
        "ax3.set_xlabel('Predicted Values')\n",
        "ax3.set_ylabel('Residuals')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Feature importance (if available)\n",
        "ax4 = axes[1, 1]\n",
        "if hasattr(models[best_model], 'feature_importances_'):\n",
        "    importances = models[best_model].feature_importances_\n",
        "    feature_names = feature_cols\n",
        "    \n",
        "    # Get top 10 features\n",
        "    top_indices = np.argsort(importances)[-10:]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    top_importances = importances[top_indices]\n",
        "    \n",
        "    ax4.barh(range(len(top_features)), top_importances)\n",
        "    ax4.set_yticks(range(len(top_features)))\n",
        "    ax4.set_yticklabels(top_features)\n",
        "    ax4.set_title(f'Top 10 Feature Importance - {best_model}')\n",
        "    ax4.set_xlabel('Importance')\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, f'{best_model}\\nFeature importance\\nnot available', \n",
        "            ha='center', va='center', transform=ax4.transAxes)\n",
        "    ax4.set_title(f'Feature Importance - {best_model}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('sales_forecasting_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, result in results.items():\n",
        "    print(f\"{model_name:15} | MAE: {result['MAE']:8.2f} | RMSE: {result['RMSE']:8.2f} | R¬≤: {result['R2']:6.4f}\")\n",
        "\n",
        "best_model = min(results.keys(), key=lambda x: results[x]['MAE'])\n",
        "print(f\"\\nüèÜ Best Model: {best_model} (Lowest MAE: {results[best_model]['MAE']:.2f})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüéâ Sales Forecasting Project Completed!\")\n",
        "print(\"‚úÖ All mandatory bonus features implemented:\")\n",
        "print(\"   - Rolling averages and statistical features\")\n",
        "print(\"   - Seasonal decomposition\")\n",
        "print(\"   - XGBoost and LightGBM models\")\n",
        "print(\"   - Time-aware cross-validation\")\n",
        "print(\"   - Comprehensive visualizations\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
